<?xml version='1.0' encoding='UTF-8'?>
<rss version='2.0'>
<channel>
<generator>
clj-rss
</generator>
<link>
https://news.ycombinator.com
</link>
<title>
myread
</title>
<description>
myread
</description>
<item>
<author>
unknown
</author>
<link>
https://blog.mozilla.org/luke/2014/01/14/asm-js-aot-compilation-and-startup-performance/
</link>
<description>
&lt;div&gt;&lt;div class=&quot;entry-content&quot;&gt;
						&lt;p&gt;With the &lt;a href=&quot;https://blog.mozilla.org/blog/2013/12/12/first-3d-commercial-web-game-powered-by-asm-js-unveiled/&quot;&gt;recent announcement&lt;/a&gt; of a commercial game shipping using &lt;a href=&quot;http://emscripten.org&quot;&gt;Emscripten&lt;/a&gt; and &lt;a href=&quot;http://asmjs.org/&quot;&gt;asm.js&lt;/a&gt;, I thought it&amp;#x2019;d be a good time to explain how asm.js is executed in Firefox and some of the load-time optimizations we&amp;#x2019;ve made since the &lt;a href=&quot;https://blog.mozilla.org/luke/2013/03/21/asm-js-in-firefox-nightly/&quot;&gt;initial landing&lt;/a&gt; of OdinMonkey in March.  (OdinMonkey is an optimization module inside Mozilla&amp;#x2019;s JavaScript engine.)  There have also been significant &lt;a href=&quot;https://hacks.mozilla.org/2013/12/gap-between-asm-js-and-native-performance-gets-even-narrower-with-float32-optimizations/&quot;&gt;throughput optimizations&lt;/a&gt; as well, but I&amp;#x2019;ll stick to load time in this post.&lt;/p&gt;
&lt;p&gt;Measuring the &lt;a href=&quot;http://www.unrealengine.com/html5&quot;&gt;Epic Citadel demo&lt;/a&gt; (based on the same Unreal Engine 3 inside &lt;a href=&quot;http://www.monstermadness.com&quot;&gt;Monster Madness&lt;/a&gt;), I see a 2x improvement:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.mozilla.org/luke/files/2014/01/citadel-warm-load.png&quot; class=&quot;aligncenter&quot;&gt;&lt;/p&gt;
&lt;p&gt;Times were measured with a simple stopwatch up to the first animation frame on a 16&amp;#xD7;2.4Ghz core Linux machine.  (An IndexedDB bug in either the demo or Chrome causes level data not to be cached so time in &amp;#x201C;Downloading data&amp;#x201D; is explicitly subtracted from Chrome&amp;#x2019;s time.)&lt;/p&gt;
&lt;p&gt;Cold load time improvements on the Citadel demo are harder to see since network latency plays a much larger part and adds considerable variance.  Measuring the &lt;a href=&quot;http://flohofwoe.net/demos.html&quot;&gt;Nebula3 demos&lt;/a&gt; instead, which have a smaller initial download size and are compiled with both Emscripten and PNaCl, we can also see significantly better load times:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.mozilla.org/luke/files/2014/01/nebula-cold-load.png&quot; class=&quot;aligncenter&quot;&gt;&lt;/p&gt;
&lt;p&gt;Times were again measured with a simple stopwatch up to first animation frame.&lt;/p&gt;
&lt;p&gt;In this blog post I&amp;#x2019;ll explain the compilation strategy we use for asm.js, why we decided to try this strategy, how it&amp;#x2019;s been working, and 3 optimizations that have had a significant impact on load time.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The post is a bit long, so here&amp;#x2019;s the TL;DR:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ahead-of-time (AOT) compilation is used for asm.js to provide predictable performance.&lt;/li&gt;
&lt;li&gt;With a few cores, parallel compilation hides most of the cost of using the top-tier compiler for all code.&lt;/li&gt;
&lt;li&gt;Async compilation allows the webapp to stay responsive during AOT compilation.&lt;/li&gt;
&lt;li&gt;Caching compiled machine code greatly improves warm start time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;JIT Compilation&lt;/h2&gt;
&lt;p&gt;Before getting into how we compile asm.js, let&amp;#x2019;s look at a diagram of the path taken by normal JavaScript in SpiderMonkey (Mozilla&amp;#x2019;s JavaScript engine).  In this diagram, boxes are data structures and arrows represent algorithms which consume and/or generate these data structures:&lt;br&gt;
&lt;a href=&quot;https://blog.mozilla.org/luke/files/2013/12/jit-diagram.png&quot;&gt;&lt;img src=&quot;https://blog.mozilla.org/luke/files/2013/12/jit-diagram.png&quot; alt=&quot;jit-diagram&quot; width=&quot;719&quot; class=&quot;aligncenter size-full wp-image-596&quot;&gt;&lt;/a&gt;&lt;br&gt;
In short, units of code (like functions, &lt;code&gt;eval&lt;/code&gt; scripts, and global scripts) start as a bunch of characters in memory and gradually get compiled into forms that are able to execute more efficiently.  While each unit of code starts the same way, different units of code will move along the arrows of this diagram at different times as they are run and judged hot enough.  This compilation strategy is generally called &lt;a href=&quot;http://en.wikipedia.org/wiki/Just-in-time_compilation&quot;&gt;Just-In-Time (JIT) compilation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Going into a little more detail on the labels in the digram:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AST: &lt;a href=&quot;http://en.wikipedia.org/wiki/Abstract_syntax_tree&quot;&gt;Abstract Syntax Tree&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Baseline: a JIT compiler that balances compilation speed and the performance of generated code (see &lt;a href=&quot;https://blog.mozilla.org/javascript/2013/04/05/the-baseline-compiler-has-landed&quot;&gt;Kannan&amp;#x2019;s post&lt;/a&gt; for details)&lt;/li&gt;
&lt;li&gt;Ion: short for IonMonkey, a JIT compiler that produces highly-optimized code at the expense of compilation speed (see &lt;a href=&quot;https://blog.mozilla.org/javascript/2012/09/12/ionmonkey-in-firefox-18/&quot;&gt;David&amp;#x2019;s post&lt;/a&gt; for details)&lt;/li&gt;
&lt;li&gt;MIR: an &lt;a href=&quot;http://en.wikipedia.org/wiki/Static_single_assignment_form&quot;&gt;SSA&lt;/a&gt;-based representation of code used throughout Ion&lt;/li&gt;
&lt;li&gt;Profile: collect metadata describing the runtime behavior of the code&lt;/li&gt;
&lt;li&gt;Ion-build: generate MIR from bytecode and profiling metadata&lt;/li&gt;
&lt;li&gt;Ion-compile: optimize and generate machine code from MIR&lt;/li&gt;
&lt;li&gt;Bail: stop executing Ion-compiled code in order to Ion-compile a new version or spend more time collecting profiling metadata in Baseline-compiled code&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given this whole process, it&amp;#x2019;s reasonable to ask: &lt;em&gt;why do we need all these tiers of execution&lt;/em&gt;?  To wit, V8 has two tiers, and Apple&amp;#x2019;s JSC has three and is experimenting with a &lt;a href=&quot;http://trac.webkit.org/wiki/FTLJIT&quot;&gt;fourth&lt;/a&gt;.  Thus, this strategy is common (although people are always looking for &lt;a href=&quot;https://wiki.openjdk.java.net/display/Graal/Publications+and+Presentations&quot;&gt;something&lt;/a&gt; &lt;a href=&quot;http://pointersgonewild.wordpress.com/higgs/&quot;&gt;simpler&lt;/a&gt;).  There are two main reasons we&amp;#x2019;ve found in SpiderMonkey for this tiered structure.&lt;/p&gt;

&lt;p&gt;One reason is that SpiderMonkey has to run many different types of code and most code doesn&amp;#x2019;t run long enough to amortize the cost of compilation.  In fact, most code &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=678037&quot;&gt;doesn&amp;#x2019;t even run once&lt;/a&gt; which is why SpiderMonkey and other JS engines wait for a function to be run before even fully parsing it.  Of code that is run, most doesn&amp;#x2019;t get warm enough to Baseline-compile and, similarly, most warm code doesn&amp;#x2019;t get hot enough to Ion-compile.  Thus, each tier of execution services a distinct type of workload.&lt;/p&gt;
&lt;p&gt;The other reason is that the Ion-build step actually &lt;em&gt;depends&lt;/em&gt; on code having warmed up in Baseline so that the profiling metadata is likely representative of future execution.  Ion compilation uses this metadata to specialize the types of values, objects, operations, etc which it could not do based on static analysis of the code alone.&lt;/p&gt;
&lt;p&gt;What&amp;#x2019;s great about this design is that it has allowed continual progress by modern JavaScript engines on all kinds of JavaScript code.  This progress continues today in all the major JS engines without signs of letting up.&lt;/p&gt;
&lt;h2&gt;JIT Problems&lt;/h2&gt;
&lt;p&gt;As it became clear that Emscripten was a big deal (remember the &lt;a href=&quot;http://badassjs.com/post/12035631618/broadway-an-h-264-decoder-in-javascript-running-at&quot;&gt;H.264 decoder&lt;/a&gt;?), we started to try it out on bigger codes and talk with potential users.  As we did this, one thing that became clear: if the web was going to be a serious porting target for large, computationally-intensive apps, we needed performance to be predictable.  Now, even with native code, performance is never &lt;em&gt;truly&lt;/em&gt; predictable due to things like dynamic scheduling and cache hierarchies.  However, with Emscripten output, we were seeing some pretty violent fluctuations in startup and throughput on differnet codes and on different browsers.&lt;/p&gt;
&lt;p&gt;Analyzing these fluctuations, we saw several causes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;JIT compilation time;&lt;/li&gt;
&lt;li&gt;as code warms up, it runs in lower-tier execution modes where it executes more slowly;&lt;/li&gt;
&lt;li&gt;compiler heuristics (such as: which scripts should be compiled with the top-tier JIT, when and where to inline, whether to compile a loop side-entry and what machine types to use to represent numbers) can make suboptimal choices that permanently reduce throughput; and&lt;/li&gt;
&lt;li&gt;some optimizations were just missing from the underlying backend compiler because they were rather difficult to implement in the general case (e.g., a better &lt;a href=&quot;http://en.wikipedia.org/wiki/Calling_convention&quot;&gt;calling convention&lt;/a&gt; for direct and indirect function calls).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each of these problems can potentially be mitigated by adding new JIT compilation techniques and heuristics.  Indeed, we&amp;#x2019;ve seen a lot of improvement along these lines in the V8 and SpiderMonkey JIT compilers in the last year and I expect to see more in the future.  For example, in both JIT compilers, a few heuristic tweaks provided large throughput improvements on the &lt;code&gt;asmjs-apps&lt;/code&gt; benchmarks on &lt;a href=&quot;http://arewefastyet.com&quot;&gt;arewefastyet.com&lt;/a&gt; and background JIT compilation has helped to significantly reduce JIT compilation pauses.&lt;/p&gt;
&lt;p&gt;However, the question is: to what &lt;em&gt;extent&lt;/em&gt; can these problems be mitigated?  Unfortunately, that&amp;#x2019;s hard to know &lt;em&gt;a priori&lt;/em&gt;: you only really know when you&amp;#x2019;re done.  Furthermore, as with any heuristic tuning problem, it&amp;#x2019;s easy to measure on workloads A, B and C only to find afterwards that the fixes don&amp;#x2019;t generalize to workloads D-Z.&lt;/p&gt;
&lt;p&gt;In broader terms: with the proliferation of walled gardens and the consequent frustration of developers, the Web has a great opportunity to provide an open, portable alternative.  But to really be an alternative for many types of applications, the web needs predictable, near-native performance.  The time is ripe, so we don&amp;#x2019;t want to miss the opportunity by blocking on a &lt;a href=&quot;http://c2.com/cgi/wiki?SufficientlySmartCompiler&quot;&gt;Sufficiently Smart Compiler&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;AOT Compilation&lt;/h2&gt;
&lt;p&gt;To attempt to solve the above problems, we started the OdinMonkey experiment.  The basic idea behind the experiment was: Emscripten-generated code has enough type information preserved from the original statically-typed source language that we can avoid all the dynamic-language compilation infrastructure and use a simple &lt;a href=&quot;http://en.wikipedia.org/wiki/AOT_compiler&quot;&gt;Ahead-of-Time (AOT) compiler&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, given the following C code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;int function f(int i) {
  return i + 1;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Emscripten would output the following JS code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;function f(i) {
  i = i|0;
  return (i + 1)|0;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The statement &amp;#x201C;&lt;code&gt;i = i|0&lt;/code&gt;&amp;#x201D; effectively performs the JS spec &lt;a href=&quot;https://people.mozilla.org/~jorendorff/es6-draft.html#sec-toint32&quot;&gt;&lt;code&gt;ToInt32&lt;/code&gt;&lt;/a&gt; on the input, ensuring that &lt;code&gt;+&lt;/code&gt; always operates on an integer.  If we can prove that all callers pass &lt;code&gt;int&lt;/code&gt;s, then this coercion is a no-op.  The expression &amp;#x201C;&lt;code&gt;(i + 1)|0&lt;/code&gt;&amp;#x201D; exactly simulates &lt;a href=&quot;http://en.wikipedia.org/wiki/2s_complement&quot;&gt;2s complement&lt;/a&gt; addition meaning that this JavaScript expression compiles to a single machine instruction &amp;#x2014; no type tests, no overflow checks.&lt;/p&gt;
&lt;p&gt;If you squint your eyes at the above code, you can view &amp;#x201C;&lt;code&gt;i = i|0&lt;/code&gt;&amp;#x201D; as a parameter type declaration, &amp;#x201C;&lt;code&gt;return (...)|0&lt;/code&gt;&amp;#x201D; as a return type declaration and binary &lt;code&gt;+&lt;/code&gt; as taking two &lt;code&gt;int&lt;/code&gt; types and returning a &lt;a href=&quot;http://asmjs.org/spec/latest/#intish&quot;&gt;special type&lt;/a&gt; which requires coercion via &lt;code&gt;ToInt32&lt;/code&gt; or &lt;code&gt;ToUint32&lt;/code&gt; before use.  This basic idea of viewing runtime coercions as types can be extended to &lt;em&gt;all&lt;/em&gt; statements and expressions in Emscripten-generated code and the resulting type system is &lt;a href=&quot;http://asmjs.org/spec/latest/&quot;&gt;asm.js&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Given the asm.js type system, OdinMonkey is easily able to generate MIR from the AST.  As an example, check out the &lt;a href=&quot;http://hg.mozilla.org/mozilla-central/file/9d593727eb94/js/src/jit/AsmJS.cpp#l4176&quot;&gt;CheckNot&lt;/a&gt; function in OdinMonkey (which checks the &lt;code&gt;!&lt;/code&gt; operator): as input it receives a &lt;code&gt;ParseNode&lt;/code&gt; (an AST node) and, as output, it returns an &lt;code&gt;MNot&lt;/code&gt; MIR node and the expression&amp;#x2019;s result type (which &lt;a href=&quot;http://asmjs.org/spec/latest/#unary-operators&quot;&gt;according to the spec&lt;/a&gt; is &lt;code&gt;int&lt;/code&gt;).  If any of the types fail to match, a type error message (like you&amp;#x2019;d expect from a C compiler) is output to the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Tools/Web_Console&quot;&gt;Web Console&lt;/a&gt; and OdinMonkey transparently falls back to normal JS compilation.&lt;/p&gt;
&lt;p&gt;In terms of the previous JIT compilation diagram, OdinMonkey adds a single new arrow between AST and MIR:&lt;br&gt;
&lt;a href=&quot;https://blog.mozilla.org/luke/files/2013/12/aot-diagram.png&quot;&gt;&lt;img src=&quot;https://blog.mozilla.org/luke/files/2013/12/aot-diagram.png&quot; alt=&quot;aot-diagram&quot; width=&quot;450&quot; class=&quot;aligncenter size-full wp-image-595&quot;&gt;&lt;/a&gt;&lt;br&gt;
Furthermore, after asm.js type checking succeeds (as well as the &lt;a href=&quot;http://asmjs.org/spec/latest/#linking-0&quot;&gt;link-time check&lt;/a&gt;), it is not possible for the generated code to take the Bail edge: there are no dynamically-checked assumptions that can fail.&lt;/p&gt;
&lt;p&gt;In addition to simplifying the compilation process, the asm.js type system also provides three broader benefits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;asm.js acts as a testable interface between Emscripten and the browser.  (This found bugs in Emscripten.)&lt;/li&gt;
&lt;li&gt;asm.js specifies a target for non-Emscripten code generators so that they don&amp;#x2019;t have to re-discover the same sweet spot as Emscripten.  (Speaking of, check out the experimental &lt;a href=&quot;http://www.rfk.id.au/blog/entry/pypy-js-poc-jit&quot;&gt;asm.js PyPy backend&lt;/a&gt; and &lt;a href=&quot;https://github.com/jlongster/LLJS&quot;&gt;LLJS-asm.js fork&lt;/a&gt;.)&lt;/li&gt;
&lt;li&gt;asm.js establishes a clear optimization target for all browsers so that this style of code can become portably fast.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;AOT Potential Problems&lt;/h2&gt;
&lt;p&gt;Despite all these advantages, AOT has a significant potential downside: it compiles everything using the most expensive compiler without knowing if the code being compiled is hot or cold.  This would obviously be a problem if an app contained a lot of cold or dead asm.js code.  Similarly, AOT would be a net loss for an app with a lot of code that runs in short bursts so that low tiers of execution and compilation stalls aren&amp;#x2019;t noticeable.  Thus, the load-time performance of AOT relative to JIT depends on the kind of code being executed.&lt;/p&gt;
&lt;p&gt;Another potential pitfall for AOT is pathologically-large functions since these can take a really long time to compile in the top-tier compiler.  With JIT compilation, the usual heuristics ensure that the top-tier compiler is never used.  With &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=875174&quot;&gt;some work&lt;/a&gt;, OdinMonkey could be extended with heuristics do the same.  In the meantime, Alon added an &amp;#x201C;&lt;a href=&quot;http://mozakai.blogspot.com/2013/08/outlining-workaround-for-jits-and-big.html&quot;&gt;outlining&lt;/a&gt;&amp;#x201D; option to Emscripten that automatically breaks up large functions and has been quite effective.  By making functions smaller, outlining also improves performance of asm.js on non-OdinMonkey since it encourages the JIT to use the top-tier compiler.&lt;/p&gt;
&lt;p&gt;One theoretical response to these load-time concerns is that the &lt;code&gt;&quot;use asm&quot;&lt;/code&gt; directive required at the beginning of any &lt;a href=&quot;http://asmjs.org/spec/latest/#modules&quot;&gt;asm.js module&lt;/a&gt; has no semantics and can simply be removed if AOT compilation is not beneficial.  As such, &lt;code&gt;&quot;use asm&quot;&lt;/code&gt; gives the developer more control over the compilation scheme used for their application.  In theory (it&amp;#x2019;s difficult in practice at the moment due to lack of automated tooling), developers can exercise even finer-grain control by choosing which functions are inside the asm.js module (and thus receive AOT compilation) and which are outside (and thus receive JIT compilation).  One can even imagine an Emscripten &lt;a href=&quot;http://en.wikipedia.org/wiki/Profile-guided_optimization&quot;&gt;PGO&lt;/a&gt; pass that does this automatically for cold code.&lt;/p&gt;
&lt;p&gt;In the end, though, it&amp;#x2019;s hard to predict what will happen in practice so we had to just try.  (OdinMonkey was started as a experiment, after all.)&lt;/p&gt;
&lt;p&gt;The results so far have been good.  In addition to those reported at the beginning of the post, cold load times are also measured by the &lt;code&gt;asmjs-&lt;wbr&gt;apps-*-&lt;wbr&gt;loadtime&lt;/wbr&gt;&lt;/wbr&gt;&lt;/code&gt; synthetic workloads on &lt;a href=&quot;http://arewefastyet.com/#machine=12&amp;amp;view=breakdown&amp;amp;suite=asmjs-apps&quot;&gt;awfy&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.mozilla.org/luke/files/2014/01/awfy-load.png&quot; class=&quot;aligncenter&quot;&gt;&lt;/p&gt;
&lt;p&gt;In this graph, Firefox (JIT) refers to Firefox&amp;#x2019;s performance with OdinMonkey disabled (by passing &lt;code&gt;--no-asmjs&lt;/code&gt; to the JS shell or setting &lt;code&gt;javascript.&lt;wbr&gt;options.&lt;wbr&gt;asmjs&lt;/wbr&gt;&lt;/wbr&gt;&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; in &lt;code&gt;about:config&lt;/code&gt; in the browser).&lt;/p&gt;
&lt;p&gt;Another data point is the &lt;a href=&quot;http://kripken.github.io/misc-js-benchmarks/banana/benchmark.html&quot;&gt;BananaBread benchmark&lt;/a&gt; which conveniently measures its own load time:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.mozilla.org/luke/files/2014/01/bananabench-cold-load.png&quot; class=&quot;aligncenter&quot;&gt;&lt;/p&gt;
&lt;p&gt;This graph reports the sum of the &amp;#x201C;preload&amp;#x201D; and &amp;#x201C;startup&amp;#x201D; times when the benchmark is run in headless mode with a cold cache.&lt;/p&gt;
&lt;p&gt;Now let&amp;#x2019;s look at the major optimizations that AOT compilation allows.&lt;/p&gt;
&lt;h2&gt;Parallel Compilation&lt;/h2&gt;
&lt;p&gt;With the intermediate JIT compilation steps avoided, the majority of AOT compilation time is in the Ion-compile step.  For example, measuring the Citadel demo we can see the following breakdown of time:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Parse AST: 1.5s&lt;/li&gt;
&lt;li&gt;Odin-build AST into MIR: 1.5s&lt;/li&gt;
&lt;li&gt;Ion-compile MIR: 8s&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Fortunately, the Ion-compile step is also the most parallelizable: each function in the &lt;a href=&quot;http://asmjs.org/spec/latest/#modules&quot;&gt;asm.js module&lt;/a&gt; results in an independent Ion compilation and there are tens of thousands of functions in large apps.  Even better, SpiderMonkey had already supported &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=774253&quot;&gt;background Ion-compilation&lt;/a&gt; for a year before OdinMonkey, so we were able to add parallel compilation to OdinMonkey &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=850070&quot;&gt;without much trouble&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After basic parallel compilation worked, we made an additional refinement to extract further parallelism.  Originally, the entire asm.js module would be parsed into one big AST before being handed over to OdinMonkey.  OdinMonkey would then simply recurse over the AST, firing off parallel Ion compilations as it went.  This was suboptimal for two reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While parsing the AST, only one core was working.&lt;/li&gt;
&lt;li&gt;Since the AST is an order of magnitude bigger than the source and asm.js source can be 35MB (don&amp;#x2019;t worry, that compresses down to 5MB over the wire with HTTP gzip content encoding), we were seeing out-of-memory errors on mobile devices and even 32-bit desktop processes with many tabs open.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The solution to both of these problems was to allow Odin-building and Ion-compiling to overlap parsing as illustrated in the following psuedo code:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;while (not at end of asm.js module) {
  ast = ParseFunction();
  mir = CheckAndEmit(ast);
  StartBackgroundIonCompilation(mir);
  ReleaseMemory(ast)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the time to Ion-compile a function is on average longer than the time to parse, the process looks something like this:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://blog.mozilla.org/luke/files/2013/12/parallel.png&quot;&gt;&lt;img src=&quot;https://blog.mozilla.org/luke/files/2013/12/parallel.png&quot; alt=&quot;parallel&quot; width=&quot;784&quot; class=&quot;aligncenter size-full wp-image-891&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To measure the effect, first disable caching (set &lt;code&gt;javascript.&lt;wbr&gt;options.&lt;wbr&gt;parallel_&lt;wbr&gt;parsing&lt;/wbr&gt;&lt;/wbr&gt;&lt;/wbr&gt;&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;) and then compare compile times with and without &lt;code&gt;javascript.&lt;wbr&gt;options.&lt;wbr&gt;ion.&lt;wbr&gt;parallel_&lt;wbr&gt;compilation&lt;/wbr&gt;&lt;/wbr&gt;&lt;/wbr&gt;&lt;/wbr&gt;&lt;/code&gt; enabled.  To get a more precise measure of compile time, look at the &amp;#x201C;total compilation time ___ms&amp;#x201D; part of the &amp;#x201C;Successfully compiled asm.js code&amp;#x201D; Web Console message.&lt;/p&gt;
&lt;p&gt;On my machine, parallel compilation reduces compile time from 11s to 5s on the Citadel demo, but this improvement is obviously contigent on the number of cores.  Measuring with 2 cores, the compile time is 9s, with 3 cores, 6s, and with 4 cores, 5s.  Adding further cores doesn&amp;#x2019;t appear to help.  The remaining gap between this and the theoretical minimum of 3s suggested above is largely due to a &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=959263&quot;&gt;fixable&lt;/a&gt; implementation detail.&lt;/p&gt;
&lt;h2&gt;Asynchronous Compilation&lt;/h2&gt;
&lt;p&gt;As described above, AOT compilation occurs when &lt;code&gt;&quot;use asm&quot;&lt;/code&gt; is first encountered while parsing.  This can be while parsing an inline &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag or the string passed to &lt;code&gt;eval&lt;/code&gt; or the &lt;code&gt;Function&lt;/code&gt; constructor.  All of these happen on the browser&amp;#x2019;s main thread and thus a large asm.js compilation will hold up event handling and the page will appear frozen (as well as the whole Firefox browser on desktop since it&amp;#x2019;s not multiprocess (&lt;a href=&quot;http://billmccloskey.wordpress.com/2013/12/05/multiprocess-firefox/&quot;&gt;yet&lt;/a&gt;!)).&lt;/p&gt;
&lt;p&gt;Since HTML allows events to be delivered to pages that still have pending &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; elements to evaluate, &lt;em&gt;any&lt;/em&gt; script may technically be parsed off the main thread.  Unfortunately, the script must still execute synchronously with respect to parsing the rest of the document and constructing the DOM so script parsing traditionally happens on the main thread right before execution.&lt;/p&gt;
&lt;p&gt;Fortunately, HTML5 added a new &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/HTML/Element/script&quot;&gt;&lt;code&gt;async&lt;/code&gt;&lt;/a&gt; property to script elements that defaults to &lt;code&gt;true&lt;/code&gt; for script-created external script elements and can be set explicitly for external scripts (&lt;code&gt;&amp;lt;script async src=&quot;...&quot;&amp;gt;&lt;/code&gt;).  When &lt;code&gt;async&lt;/code&gt; is &lt;code&gt;true&lt;/code&gt;, the browser is allowed to evaluate the script whenever it wants.  This makes &lt;code&gt;async&lt;/code&gt; scripts a perfect candidate for parsing off the main thread and Brian Hackett recently &lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=906371&quot;&gt;made it happen&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;OdinMonkey, by nature of running at parse-time, got to ride along for free(-ish).  Even better, most Emscripten&amp;#x2019;d apps are already async since Emscripten&amp;#x2019;s default harness uses an async script to load the main asm.js module.  See &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Games/Techniques/Async_scripts&quot;&gt;this short MDN article&lt;/a&gt; for more details, gotchas and workarounds concerning async scripts.&lt;/p&gt;
&lt;h2&gt;Caching&lt;/h2&gt;
&lt;p&gt;When someone starts contributing to SpiderMonkey, there are a few ideas they will almost inevitably have.  One is: &amp;#x201C;Why don&amp;#x2019;t we cache JIT code?&amp;#x201D;. The response to this question is usually some combination of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;JIT compilation is pretty fast and usually a small percentage of total run time, so it probably wouldn&amp;#x2019;t help most sites.&lt;/li&gt;
&lt;li&gt;The implementation would be really complicated because JIT code is highly dependent on the current browser state in memory and JIT code forms a complex graph data structure.&lt;/li&gt;
&lt;li&gt;It&amp;#x2019;s hard to know when and what to cache; compilation is happening all the time in tiny units.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;None of these problems are insurmountable, but together they make JIT-code caching a fairly daunting task. (To wit, the other inevitable question is &amp;#x201C;Why don&amp;#x2019;t we use LLVM as a compiler backend?&amp;#x201D;, so it&amp;#x2019;s great to see Apple actually &lt;a href=&quot;http://trac.webkit.org/wiki/FTLJIT&quot;&gt;trying this&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;In contrast, for asm.js code the cost/benefit analysis is much simpler:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;compilation time is significant&lt;/li&gt;
&lt;li&gt;the asm.js module has limited and explicit dependencies (viz., the arguments to the &lt;a href=&quot;http://asmjs.org/spec/latest/#modules&quot;&gt;asm.js module function&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;the representation of the generated asm.js module is relatively simple and easily &lt;a href=&quot;http://hg.mozilla.org/mozilla-central/file/27228e525e08/js/src/jit/AsmJSModule.cpp#l662&quot;&gt;serialized and deserialized&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;making caching a clear win.  So that&amp;#x2019;s what we did.
&lt;/p&gt;
&lt;p&gt;There is one unfortunate limitation in the current implementation, though: caching only kicks in for &lt;code&gt;async&lt;/code&gt; scripts and WebWorker code (due to some hopefully temporary main-thread limitations arising from browser storage integration).  Thus, large applications have &lt;em&gt;two&lt;/em&gt; big reasons to use &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Games/Techniques/Async_scripts&quot;&gt;async scripts&lt;/a&gt;.  Other than that, the cache should behave predictably according to the following rules:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The cache is only used for medium-to-large modules (the current cutoff is modules longer than 10,000 characters).&lt;/li&gt;
&lt;li&gt;The cache entry is keyed on: the origin of the script, the source characters of the asm.js module, the type of CPU and its features, the Firefox build-id (which changes on every major or minor release).&lt;/li&gt;
&lt;li&gt;The asm.js cache participates in browser-wide quota management such that, when total temporary storage grows past a certain threshold, storage is evicted on an LRU basis.&lt;/li&gt;
&lt;li&gt;There is a fixed cap (currently 16) on the number of cached asm.js modules per origin; eviction is LRU.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To get confirmation of caching, open the Web Console: the asm.js success messages now include a &amp;#x201C;loaded from cache&amp;#x201D; / &amp;#x201C;stored in cache&amp;#x201D; / &amp;#x201C;not stored in cache&amp;#x201D; clause.&lt;/p&gt;
&lt;p&gt;To see caching in action, try out the demos mentioned in the introduction with and without &lt;code&gt;javascript.&lt;wbr&gt;options.&lt;wbr&gt;parallel_&lt;wbr&gt;parsing&lt;/wbr&gt;&lt;/wbr&gt;&lt;/wbr&gt;&lt;/code&gt; enabled in &lt;code&gt;about:config&lt;/code&gt; (&lt;code&gt;true&lt;/code&gt; enables caching).  Using this to measure cached vs. uncached warm load time of the Epic Citadel demo shows a 2x improvement:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog.mozilla.org/luke/files/2014/01/citadel-warm-cached-load.png&quot; class=&quot;aligncenter&quot;&gt;&lt;/p&gt;
&lt;p&gt;As before, times were measured with a simple stopwatch up to the first animation frame on a 16&amp;#xD7;2.4Ghz core Linux machine.&lt;/p&gt;
&lt;p&gt;Note: to clear out the asm.js cache for a given origin, click the &lt;a href=&quot;https://support.mozilla.org/en-US/kb/how-do-i-tell-if-my-connection-is-secure&quot;&gt;Site Identity Button&lt;/a&gt; &amp;#x2192; More Information &amp;#x2192; Permissions &amp;#x2192; (scroll down) &amp;#x2192; Clear Storage.  (I hear better quota management UI is coming.)&lt;/p&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;OdinMonkey started as an experiment in achieving predictable, high performance through AOT compilation.  With the rapid pace of innovation in JS JIT compilation techniques, it&amp;#x2019;s definitely too early to draw any final conclusions.  However, almost a year after the initial release, OdinMonkey is still showing marked leads in predictability, throughput and load time.&lt;/p&gt;
&lt;p&gt;In recent news, it&amp;#x2019;s interesting to see that Google &lt;a href=&quot;http://www.androidpolice.com/2013/11/06/meet-art-part-1-the-new-super-fast-android-runtime-google-has-been-working-on-in-secret-for-over-2-years-debuts-in-kitkat/&quot;&gt;just shipped ART&lt;/a&gt;, an AOT compiler for Java on Android (the current Java VM on Android, Dalvik, is a JIT compiler).  OdinMonkey and ART aren&amp;#x2019;t really comparable for several reasons, but some of the arguments made in the article about startup time definitely sound familiar &amp;#x263A;.&lt;/p&gt;
&lt;p&gt;On a final note, I&amp;#x2019;d like to emphasize that the majority of Mozilla&amp;#x2019;s JavaScript performance engineers are still focused on improving &lt;em&gt;general&lt;/em&gt; JavaScript performance &lt;sup&gt;&lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=generationalgc&quot;&gt;1&lt;/a&gt;,&lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=580070&quot;&gt;2&lt;/a&gt;,&lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=774253&quot;&gt;3&lt;/a&gt;,&lt;a href=&quot;https://blog.mozilla.org/javascript/2013/04/05/the-baseline-compiler-has-landed&quot;&gt;4&lt;/a&gt;,&lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=906371&quot;&gt;5&lt;/a&gt;,&lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=888109&quot;&gt;6&lt;/a&gt;,&lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=860923&quot;&gt;7&lt;/a&gt;,&lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=903457&quot;&gt;8&lt;/a&gt;,&lt;a href=&quot;https://bugzilla.mozilla.org/show_bug.cgi?id=782913&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.  Moreover, I think the future we&amp;#x2019;re moving toward has web apps composed of both compiled modules and high-level handwritten code.  &lt;a href=&quot;https://playcanvas.com/&quot;&gt;PlayCanvas&lt;/a&gt; provides us an early &lt;a href=&quot;http://apps.playcanvas.com/playcanvas/physics/charactercontroller&quot;&gt;example&lt;/a&gt; of this, embedding &lt;a href=&quot;https://github.com/kripken/ammo.js/&quot;&gt;ammo.js&lt;/a&gt; (an Emscripten port of the Bullet physics engine) into an existing hand-written game engine.  I hope to see this trend continue with more reusable compiled components in more application domains and with tighter integration between compiled and handwritten JS (e.g. &lt;a href=&quot;http://lljs.org&quot;&gt;LLJS&lt;/a&gt;, &lt;a href=&quot;https://github.com/kripken/emscripten/wiki/embind&quot;&gt;embind&lt;/a&gt;).&lt;/p&gt;
											&lt;/div&gt;

					&lt;/div&gt;
</description>
<title>
Asm.js AOT compilation and startup performance
</title>
</item>
<item>
<author>
unknown
</author>
<link>
http://chrome.blogspot.com/2014/01/everyone-can-now-track-down-noisy-tabs.html
</link>
<description>
&lt;div&gt;&lt;div class=&quot;post-body&quot;&gt;
&lt;div class=&quot;post-content&quot;&gt;

November&amp;#x2019;s &lt;a href=&quot;http://chrome.blogspot.com/2013/11/track-down-those-noisy-tabs.html&quot;&gt;Chrome beta&lt;/a&gt; introduced improvements to help make your browsing quieter, safer and sleeker. In the latest &lt;a href=&quot;http://google.com/chrome&quot;&gt;Chrome release&lt;/a&gt; we&amp;#x2019;re making these available to everyone:&lt;br&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;You can now track down noisy tabs: &lt;/b&gt;You can now visually scan your tabs for a speaker icon to quickly find the ones singing in the background. You&amp;#x2019;ll also be able to see which tabs are currently using your webcam or are being &lt;a href=&quot;http://googleblog.blogspot.com/2013/07/from-tvs-to-tablets-everything-you-love.html&quot;&gt;cast to your TV&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;div&gt;
&lt;a href=&quot;http://3.bp.blogspot.com/-cNPtAHx0f4A/UtRZu4lCvsI/AAAAAAAAAMc/cwiLUeuRzmE/s1600/noisy-tabs-sounds.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://3.bp.blogspot.com/-cNPtAHx0f4A/UtRZu4lCvsI/AAAAAAAAAMc/cwiLUeuRzmE/s200/noisy-tabs-sounds.png&quot; width=&quot;200&quot;&gt;&lt;/a&gt;&lt;i&gt;Playing audio
&lt;/i&gt;&lt;/div&gt;

&lt;/td&gt;&lt;td&gt;&lt;div&gt;
&lt;i&gt;&lt;a href=&quot;http://3.bp.blogspot.com/-7iLHDJl4Ho0/UtRZxbiXtXI/AAAAAAAAAMk/vaHmnFPgOd4/s1600/noisy-tabs-webcam.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://3.bp.blogspot.com/-7iLHDJl4Ho0/UtRZxbiXtXI/AAAAAAAAAMk/vaHmnFPgOd4/s200/noisy-tabs-webcam.png&quot; width=&quot;200&quot;&gt;&lt;/a&gt;Using your webcam
&lt;/i&gt;&lt;/div&gt;
&lt;/td&gt;&lt;td&gt;&lt;div&gt;
&lt;i&gt;&lt;a href=&quot;http://4.bp.blogspot.com/-yDQ5YuD3zpw/UtRZ1Zv64kI/AAAAAAAAAMs/v132s4QrzM0/s1600/noisy-tabs---cast.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://4.bp.blogspot.com/-yDQ5YuD3zpw/UtRZ1Zv64kI/AAAAAAAAAMs/v132s4QrzM0/s200/noisy-tabs---cast.png&quot; width=&quot;200&quot;&gt;&lt;/a&gt;Casting to your TV&lt;/i&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Safe Browsing's malware warning has gotten stronger:&lt;/b&gt; If you see this message in the download tray at the bottom of your screen, you can click &amp;#x201C;Dismiss&amp;#x201D; knowing that Chrome is &lt;a href=&quot;http://chrome.blogspot.com/2013/10/dont-mess-with-my-browser.html&quot;&gt;working to keep you safe&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;img src=&quot;https://lh5.googleusercontent.com/tQiIsL_6VNEJNH1PhrlKKKPCBa05O6q3GanYwh-DDAfbSVCEX_GdgrMgKMKUPcCFnPfbjWP9aKdVegwLDK44Ke0E9UBp7ik6s8RKtzkkU3YOTq2H9OJVqItb&quot; width=&quot;320&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Try out &lt;a href=&quot;http://chrome.blogspot.com/2013/10/a-beta-preview-supervised-users.html&quot;&gt;supervised users&lt;/a&gt; for your family members: &lt;/b&gt;You can now use a beta preview of &lt;a href=&quot;https://support.google.com/chrome/?p=ui_supervised_users&quot;&gt;supervised users&lt;/a&gt; to help family members who may need some guidance browsing the web. Once you create a supervised user, you can visit &lt;a href=&quot;http://chrome.com/manage&quot;&gt;chrome.com/manage&lt;/a&gt; to review their browsing activity and determine site restrictions.&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;
&lt;a href=&quot;http://2.bp.blogspot.com/-oY0DMdr8V_8/UtTx3nHsGsI/AAAAAAAAAM8/2CGZCMVyzuQ/s1600/supervisedusers_manage.png&quot;&gt;&lt;img border=&quot;0&quot; src=&quot;http://2.bp.blogspot.com/-oY0DMdr8V_8/UtTx3nHsGsI/AAAAAAAAAM8/2CGZCMVyzuQ/s400/supervisedusers_manage.png&quot; width=&quot;400&quot;&gt;&lt;/a&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Chrome on Windows 8 &amp;#x201C;Metro&amp;#x201D; mode gets a new look: &lt;/b&gt;Manage multiple Chrome windows and quickly get to your favorite Chrome Apps with an integrated &lt;a href=&quot;https://support.google.com/chrome_webstore/answer/3060053?hl=en&quot;&gt;app launcher&lt;/a&gt;. &amp;#xA0;On the desktop, we&amp;#x2019;ve updated the default styling of UI elements like form controls and scrollbars to match the sleek design of the new Chrome Metro interface.&lt;/li&gt;
&lt;/ul&gt;

&lt;span id=&quot;docs-internal-guid-54f2904c-91bc-9f00-321b-e33999dc591e&quot;&gt;&lt;img src=&quot;https://lh5.googleusercontent.com/Wo4xPd_Z6SJMKn-CnYqnoJbXXJrhB2Oy8_Uv3fINJ0kH4KSaerAIHqr5kRhroPiNj-H3gpo3KLINsRJ2iAXtn11s9VFfTXGIsD09AzY9Qg3GYv8ifZKt-RECbA&quot; width=&quot;400&quot;&gt;&lt;/span&gt;
Yuri Wiitala, Software Engineer and Tenacious Tab Tracker&lt;br&gt;

&lt;/div&gt;
&lt;/div&gt;


&lt;/div&gt;
</description>
<title>
Everyone can now track down noisy tabs
</title>
</item>
<item>
<author>
unknown
</author>
<link>
http://www.cc.gatech.edu/news/georgia-tech-researchers-reveal-phrases-pay-kickstarter
</link>
<description>
&lt;div&gt;&lt;div class=&quot;content&quot;&gt;

    &lt;span class=&quot;newsDate&quot;&gt;January 14, 2014&lt;/span&gt;    

    
    
    &lt;p&gt;Researchers at Georgia Tech studying the burgeoning phenomenon of crowdfunding have learned that the language used in online fundraising hold surprisingly predictive power about the success of such campaigns.&amp;#xA0;&lt;/p&gt;
&lt;p&gt;As part of their study of more than 45,000 projects on Kickstarter, Assistant Professor Eric Gilbert and doctoral candidate Tanushree Mitra reveal dozens of phrases that pay and a few dozen more that may signal the likely failure of a crowd-sourced effort.&amp;#xA0;&lt;/p&gt;
&lt;p&gt;&amp;#x201C;Our research revealed that the phrases used in successful Kickstarter campaigns exhibited general persuasion principles,&amp;#x201D; said Gilbert, who runs the Comp. Social Lab at Georgia Tech. &amp;#x201C;For example, those campaigns that follow the concept of reciprocity &amp;#x2013; that is, offer a gift in return for a pledge &amp;#x2013; and the perceptions of social participation and authority, generated the greatest amount of funding.&amp;#x201D;&amp;#xA0;&lt;/p&gt;
&lt;p&gt;While offering donors a gift may improve a campaign&amp;#x2019;s success, the study found the language project creators used to express the reward made the difference. For example, the phrases &amp;#x201C;also receive two,&amp;#x201D; &amp;#x201C;has pledged&amp;#x201D; and &amp;#x201C;project will be&amp;#x201D; strongly foretell that a project will reach funding status, while phrases such as &amp;#x201C;dressed up,&amp;#x201D; &amp;#x201C;not been able&amp;#x201D; and &amp;#x201C;trusting&amp;#x201D; are attached to unfunded projects.&amp;#xA0;&lt;/p&gt;
&lt;p&gt;The researchers examined the success of Pebble, which is the most successful Kickstarter campaign to date with more than $10 million in pledges, and compared it to Ninja Baseball, a well-publicized PC game that only earned a third of its $10,000 goal.&amp;#xA0;&lt;/p&gt;
&lt;p&gt;&amp;#x201C;The discrepancy in funding success between projects like Pebble and Ninja Baseball prompted us to consider why some projects meet funding goals and others do not,&amp;#x201D; Mitra said. &amp;#x201C;We found that the driving factors in crowdfunding ranged from social participation to encouragement to gifts &amp;#x2013; all of which are distinguished by the language used in the project description.&amp;#x201D;&amp;#xA0;&lt;/p&gt;
&lt;p&gt;For their research, Gilbert and Mitra assembled a list of all Kickstarter projects launched as of June 2, 2012, and had reached their last date of fund collection. Of the more than 45,000 projects, 51.53 percent were successfully funded while 48.47 percent were not.&amp;#xA0;&lt;/p&gt;
&lt;p&gt;After controlling for variables such as funding goals, video, social media connections, categories and pledge levels, the researchers focused on more than 20,000 phrases before compiling a dictionary of more than 100 phrases with predictive powers of success or failure.&amp;#xA0;&lt;/p&gt;
&lt;p&gt;The research suggested that the language used by creators to pitch their project plays a major role in driving the project&amp;#x2019;s success, accounting for 58.56 percent of the variance around success. The language generally fit into the following categories:&amp;#xA0;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reciprocity&lt;/strong&gt; or the tendency to return a favor after receiving one as evidenced by phrases such as &amp;#x201C;also receive two,&amp;#x201D; &amp;#x201C;pledged will&amp;#x201D; and &amp;#x201C;good karma and&lt;strong&gt;.&amp;#x201D;&lt;/strong&gt;&amp;#xA0;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scarcity &lt;/strong&gt;or attachment to something rare as shown with &amp;#x201C;option is&amp;#x201D; and &amp;#x201C;given the chance.&amp;#x201D;&amp;#xA0;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Social Proof&lt;/strong&gt;, which suggests that people depend on others for social cues on how to act as shown by the phrase &amp;#x201C;has pledged.&amp;#x201D;&amp;#xA0;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Social Identity&lt;/strong&gt; or the feeling of belonging to a specific social group. Phrases such as &amp;#x201C;to build this&amp;#x201D; and &amp;#x201C;accessible to the&amp;#x201D; fit this category.&amp;#xA0;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Liking&lt;/strong&gt;, which reflects the fact that people comply with people or products that appeal to them.&amp;#xA0;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Authority&lt;/strong&gt;, where people resort to expert opinions for making efficient and quick decisions as shown by phrases such as &amp;#x201C;we can afford&amp;#x201D; and &amp;#x201C;project will be.&amp;#x201D;&amp;#xA0;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The team&amp;#x2019;s findings are summarized in the paper &amp;#x201C;The Language that Gets People to Give: Phrases that Predict Success on Kickstarter.&amp;#x201D; The &lt;a href=&quot;http://comp.social.gatech.edu/papers/cscw14.crowdfunding.mitra.pdf&quot; rel=&quot;nofollow&quot;&gt;paper&lt;/a&gt; will be formally presented at the &lt;a href=&quot;http://confer.csail.mit.edu/cscw2014/paper#cscw339&quot; rel=&quot;nofollow&quot;&gt;17th ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW 2014)&lt;/a&gt; to be held in Baltimore, Md., from Feb. 15 to 19.&lt;/p&gt;
    
  &lt;/div&gt;

  
&lt;/div&gt;
</description>
<title>
Georgia Tech Researchers Reveal Phrases that Pay on Kickstarter
</title>
</item>
<item>
<author>
unknown
</author>
<link>
http://drj11.wordpress.com/2013/09/01/on-compiling-34-year-old-c-code/
</link>
<description>
&lt;div&gt;&lt;h2 id=&quot;post-1031&quot;&gt;On compiling 34 year old C&amp;#xA0;code&lt;/h2&gt;
			&lt;div class=&quot;entry&quot;&gt;
				&lt;p&gt;The 34 year old C code is the C code for &lt;a href=&quot;https://github.com/v7unix/v7unix/blob/f7c0e507ac6c4b96a1750f6ea4f57785a6dabb01/v7/usr/src/cmd/ed.c&quot;&gt;ed&lt;/a&gt; and &lt;a href=&quot;https://github.com/v7unix/v7unix/tree/f7c0e507ac6c4b96a1750f6ea4f57785a6dabb01/v7/usr/src/cmd/sed&quot;&gt;sed&lt;/a&gt; from Unix Version 7. I&amp;#x2019;ve been getting it to compile on a modern POSIXish system.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/v7unix/v7unix/commits/e588aaebfc4fb2af0382de519ff5000d3b2fa7cb&quot;&gt;Some changes had to be made&lt;/a&gt;. But not very many.&lt;/p&gt;
&lt;h2&gt;The union hack&lt;/h2&gt;
&lt;p&gt;sed uses &lt;a href=&quot;https://github.com/v7unix/v7unix/blob/f7c0e507ac6c4b96a1750f6ea4f57785a6dabb01/v7/usr/src/cmd/sed/sed.h#L104&quot;&gt;a union to save space in a struct&lt;/a&gt;. Specifically, the &lt;var&gt;reptr&lt;/var&gt; union has two sub structs that differ only in that one of them has a &lt;tt&gt;char *re1&lt;/tt&gt; field where the other has a &lt;tt&gt;union reptr *lb1&lt;/tt&gt;. In the old days it was possible to access members of structs inside unions without having to name the intermediate struct. For example &lt;a href=&quot;https://github.com/v7unix/v7unix/blob/f7c0e507ac6c4b96a1750f6ea4f57785a6dabb01/v7/usr/src/cmd/sed/sed0.c#L151&quot;&gt;the code in the sed implementation uses &lt;tt&gt;rep-&amp;gt;ad1&lt;/tt&gt;&lt;/a&gt; instead of &lt;tt&gt;rep-&amp;gt;reptr1.ad1&lt;/tt&gt;. That&amp;#x2019;s no longer possible (I&amp;#x2019;m pretty sure this shortcut was already out of fashion by the time K&amp;amp;R was published in 1978, but I don&amp;#x2019;t have a copy to hand).&lt;/p&gt;
&lt;p&gt;I first &lt;a href=&quot;https://github.com/v7unix/v7unix/blob/cc1c904848a2781f51dcde153b60519af8ba458a/v7/usr/src/cmd/sed/sed.h#L104&quot;&gt;changed the union to a struct that had a union inside it only for the two members that differed&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;
struct	reptr {
		char	*ad1;
		char	*ad2;
	union {
		char	*re1;
		struct reptr	*lb1;
        } u;
		char	*rhs;
		FILE	*fcode;
		char	command;
		char	gfl;
		char	pfl;
		char	inar;
		char	negfl;
} ptrspace[PTRSIZE], *rep;
&lt;/pre&gt;
&lt;p&gt;The meant changing a few &amp;#x201C;union reptr&amp;#x201D; to &amp;#x201C;struct reptr&amp;#x201D;, but most of the member accesses would be unchanged. &lt;tt&gt;-&amp;gt;re1&lt;/tt&gt; had to be changed to &lt;tt&gt;-&amp;gt;u.re1&lt;/tt&gt;, but that&amp;#x2019;s a simple search and replace.&lt;/p&gt;
&lt;p&gt;It wasn&amp;#x2019;t until I was explaining this ghastly use of union to &lt;a href=&quot;https://twitter.com/paul_furley&quot;&gt;Paul&lt;/a&gt; a day later that I realised the union is a completely unnecessary space-saving micro-optimisation. We can just have a plain struct where only one of the two fields &lt;var&gt;re1&lt;/var&gt; and &lt;var&gt;lb1&lt;/var&gt; were ever used. &lt;a href=&quot;https://github.com/v7unix/v7unix/blob/e588aaebfc4fb2af0382de519ff5000d3b2fa7cb/v7/usr/src/cmd/sed/sed.h#L104&quot;&gt;That&amp;#x2019;s much nicer, and so is the code&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;The rise of headers&lt;/h2&gt;
&lt;p&gt;In K&amp;amp;R C if the function you were calling returned &lt;tt&gt;int&lt;/tt&gt; then you didn&amp;#x2019;t need to declare it before calling it. Many functions that in modern times return &lt;tt&gt;void&lt;/tt&gt;, used to return &lt;tt&gt;int&lt;/tt&gt; (which is to say, they didn&amp;#x2019;t declare what they returned, so it defaulted to &lt;tt&gt;int&lt;/tt&gt;, and if the function used plain &lt;tt&gt;return;&lt;/tt&gt; then that was okay as long as the caller didn&amp;#x2019;t use the return value). &lt;var&gt;exit()&lt;/var&gt; is such a function. sed calls it without declaring it first, and that generates a warning:&lt;/p&gt;
&lt;pre&gt;
sed0.c:48:3: warning: incompatible implicit declaration of built-in function &amp;#x2018;exit&amp;#x2019; [enabled by default]
&lt;/pre&gt;
&lt;p&gt;I could declare &lt;var&gt;exit()&lt;/var&gt; explicitly, but it seemed simpler to just add &lt;tt&gt;#include &amp;lt;stdlib.h&amp;gt;&lt;/tt&gt;. And it is.&lt;/p&gt;
&lt;p&gt;ed declares some of the library functions it uses explicitly. &lt;a href=&quot;https://github.com/v7unix/v7unix/blob/8c4545b59cd2ae641f1f151f76837567ac563653/v7/usr/src/cmd/ed.c#L97&quot;&gt;Like &lt;var&gt;malloc()&lt;/var&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;char	*malloc();&lt;/pre&gt;
&lt;p&gt;That&amp;#x2019;s a problem because the declaration of &lt;var&gt;malloc()&lt;/var&gt; has changed. Now it&amp;#x2019;s &lt;tt&gt;void *malloc(size_t)&lt;/tt&gt;. This is a fatal violation of the C standard that the compiler is not obliged to warn me about, but thankfully it does.&lt;/p&gt;
&lt;p&gt;The modern fix is again to add header files. Amusingly, version 7 Unix didn&amp;#x2019;t even have a header file that declared &lt;var&gt;malloc()&lt;/var&gt;.&lt;/p&gt;
&lt;p&gt;When it comes to &lt;var&gt;mktemp()&lt;/var&gt; (which is also declared explicitly rather than via a header file), &lt;a href=&quot;https://github.com/v7unix/v7unix/blob/8c4545b59cd2ae641f1f151f76837567ac563653/v7/usr/src/cmd/ed.c#L146&quot;&gt;ed has a problem&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;
tfname = mktemp(&quot;/tmp/eXXXXX&quot;);
&lt;/pre&gt;
&lt;p&gt;2 problems in fact. One is that modern &lt;var&gt;mktemp()&lt;/var&gt; expects its argument to have 6 &amp;#x201C;X&amp;#x201D;s, not 5. But the more serious problem is that the storage pointed to by the argument will be modified, and the string literal that is passed is not guaranteed to be placed in a modifiable data region. I&amp;#x2019;m surprised this worked in Version 7 Unix. These days not only is it not guaranteed to work, it doesn&amp;#x2019;t actually work. SEGFAULT because &lt;var&gt;mktemp()&lt;/var&gt; tries to write into a read-only page.&lt;/p&gt;
&lt;p&gt;And the 3rd problem is of course that &lt;var&gt;mktemp()&lt;/var&gt; is a problematic interface so the better &lt;var&gt;mkstemp()&lt;/var&gt; interface made it into the POSIX standard and &lt;var&gt;mktemp()&lt;/var&gt; did not.&lt;/p&gt;
&lt;p&gt;Which brings me to&amp;#x2026;&lt;/p&gt;
&lt;h2&gt;Unfashionable interfaces&lt;/h2&gt;
&lt;p&gt;ed uses &lt;var&gt;gtty()&lt;/var&gt; and &lt;var&gt;stty()&lt;/var&gt; to get and set the terminal flags (to switch off ECHO while the encryption key is read from the terminal). Not only is &lt;var&gt;gtty()&lt;/var&gt; unfashionable in modern Unixes (I replaced it with &lt;a href=&quot;https://github.com/v7unix/v7unix/blob/master/v7/usr/src/cmd/ed.c#L1668&quot;&gt;tcgettattr()&lt;/a&gt;), it was unfashionable in Version 7 Unix. It&amp;#x2019;s not documented in the man pages; instead, &lt;var&gt;tty(4)&lt;/var&gt; documents the use of the &lt;tt&gt;TIOCGETP&lt;/tt&gt; command to &lt;var&gt;ioctl()&lt;/var&gt;.&lt;/p&gt;
&lt;p&gt;ed is already using a legacy interface in 1979.&lt;/p&gt;




&lt;div id=&quot;jp-post-flair&quot; class=&quot;sharedaddy sd-like-enabled&quot;&gt;&lt;div class=&quot;sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded&quot; id=&quot;like-post-wrapper-258145-1031-52d62825f1e98&quot;&gt;&lt;h3 class=&quot;sd-title&quot;&gt;Like this:&lt;/h3&gt;&lt;p class=&quot;likes-widget-placeholder post-likes-widget-placeholder&quot;&gt;&lt;span class=&quot;button&quot;&gt;&lt;span&gt;Like&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;loading&quot;&gt;Loading...&lt;/span&gt;&lt;/p&gt;&lt;a class=&quot;sd-link-color&quot;&gt;&lt;/a&gt;&lt;/div&gt;
&lt;/div&gt;
				
				

			&lt;/div&gt;
		&lt;/div&gt;
</description>
<title>
On compiling 34 year old C code
</title>
</item>
<item>
<author>
unknown
</author>
<link>
http://www.eurogamer.net/articles/digitalfoundry-2014-secret-developers-wii-u-the-inside-story
</link>
<description>
&lt;div&gt;&lt;article class=&quot;hd&quot;&gt;
	
													
							
						&lt;p class=&quot;byline&quot;&gt;			
							&lt;strong&gt;By&lt;/strong&gt;
							&lt;a href=&quot;http://www.eurogamer.net/author/1162&quot;&gt;The Secret Developers&lt;/a&gt;
		
							&lt;strong&gt;Published&lt;/strong&gt;
							&lt;span&gt;
																	Saturday, 11 January 2014															&lt;/span&gt;
						&lt;/p&gt;
	
																
		
		
			&lt;section class=&quot;&quot;&gt;
  &lt;p&gt;
    &lt;em&gt;The Secret Developers is Digital Foundry's occasional series where game-makers come forward to talk with us - and you - about topics they are passionate about, or in the case of this article, to give you the inside story behind a particular hot topic. As the future of the Wii U looks uncertain in the face of the successful launches for both Xbox One and PlayStation 4, this &quot;warts and all&quot; tale from a respected third-party creator gives you some idea of how Nintendo handled the transition to the high-definition gaming era, and the challenges developers faced in bringing their games to the Wii U platform.&lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;I was there when Nintendo first pitched the Wii U to developers, I worked on the hardware extensively and helped to produce one of the better third-party titles. Now, as the fate of the hardware looks uncertain after a second Christmas of disappointing sales, I wanted to tell the story of what it was actually like to work with the console, and with Nintendo, and perhaps give some context to the mixed fortunes of the machine and its third-party titles.&lt;/p&gt;
  &lt;p&gt;But first, let's go back to the beginning. The genesis of a new games console generally follows a standard pattern. Initially there is a prolonged period of research and development internally within a manufacturer where the goals and hardware designs are sketched out. These then go through a process of refinement with the hardware parts manufacturers, based on their technology and, obviously, cost.&lt;/p&gt;
  &lt;p&gt;Once the basic hardware design has been thrashed out, the internal software (SDK) teams get involved in writing the initial code/drivers and tests that are required to run the hardware. Once the teams are happy with the hardware, cost and timelines, the companies start to go out and talk to developers about the new hardware.&lt;/p&gt;
  &lt;p&gt;To begin with this will be first-party developers and feedback will be gathered that may, or may not, affect the design of the hardware. At this stage the hardware design can be changed, but the window of opportunity is getting smaller. The hardware parts manufacturers have to ramp up their production lines to produce the silicon, which takes time.&lt;/p&gt;
  &lt;p&gt;After initial feedback, the studio 'tours' begin, talking to select third-party publishers, the Ubisoft, Take-Two and EAs of the world, that the platform holders need to entice to make games for their consoles. Without games, and the income that they provide, the console soon starts to lose money, becoming a noose around the manufacturer's neck.&lt;/p&gt;
  &lt;p&gt;Major changes at this point are rare, unless they are things that can be altered through software changes (clock speeds, system OS time-slices, etc.) or can be 'easily' added to the hardware design, for example swapping out one set of memory modules for another of higher capacity.&lt;/p&gt;
  &lt;p&gt;That's where I come in.&lt;/p&gt;
  &lt;div class=&quot;pullquote  left &quot;&gt;&lt;p&gt;&quot;After initial feedback, the studio 'tours' begin, talking to the Ubisoft, Take-Two and EAs of the world... Without games, and the income that they provide, the console soon starts to lose money, becoming a noose around the manufacturer's neck.&quot; &lt;/p&gt; &lt;/div&gt;
  &lt;div class=&quot;image-gallery &quot;&gt;
    &lt;div class=&quot;illustration    &quot;&gt;
      &lt;img src=&quot;http://images.eurogamer.net/2013/articles//a/1/6/4/5/9/4/4/wii_u.jpg.jpg/EG11/resize/600x-1/quality/80/format/jpg&quot; width=&quot;600&quot; height=&quot;339.2578125&quot; alt=&quot;wiiu&quot;&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
&lt;/section&gt;&lt;section class=&quot;&quot;&gt;
  &lt;h2&gt;The reveal and post-reveal catch-up&lt;/h2&gt;
  &lt;p&gt;When I was told that Nintendo had come into the office for a meeting I could already guess as to what they were going to be talking about. Rumours had been circulating for weeks of new hardware, but nothing concrete had been said. After signing the various NDAs we all gathered in a room to hear the presentation.&lt;/p&gt;
  &lt;p&gt;It started off in the usual way with a look back on how successful the Wii had been and what their intentions were for the new hardware. They wanted a console that was the same size as the Wii and wouldn't make much noise, so &quot;mum wouldn't mind having it in the living room&quot;. It was during this statement that quiet alarm bells started to ring in my brain, but I ignored them and continued watching the presentation. The pitch then moved on to the usual &quot;we need your help to ensure that the Wii U is a success and you can help us (Nintendo) along the way&quot;. These words ended up having more significance than either we, or the presenters, could have envisaged.&lt;/p&gt;
  &lt;p&gt;Then the new controller was shown as a dummy prototype, complete with a glossy video showing how it could be used in games as a series of mock-ups, which looked exciting. By this point we were all considering how we could use the controller in our games. But then they revealed the internal details of the console and I realised the reason for my earlier alarm bells. If Nintendo wanted the hardware to have a small footprint and be quiet, they needed minimal fan noise, meaning that cooling was limited, which in turn meant that the CPU would have to produce a minimal amount of heat, which meant that the clock speed would have to be kept low. While I can't confirm specific details, the &lt;a href=&quot;http://en.wikipedia.org/wiki/Espresso_(microprocessor)&quot;&gt;collective thoughts of the internet&lt;/a&gt; are presented for reference on Wikipedia.&lt;/p&gt;
  &lt;p&gt;So a basic comparison/calculation makes the Wii U look, on paper at least, significantly &lt;em&gt;slower&lt;/em&gt; than an Xbox 360 in terms of raw CPU. This point was raised in the meeting, but the Nintendo representatives dismissed it saying that the &quot;low power consumption was more important to the overall design goals&quot; and that &quot;other CPU features would improve the performance over the raw numbers&quot;.&lt;/p&gt;
  &lt;div class=&quot;pullquote  left &quot;&gt;&lt;p&gt;&quot;Almost immediately after the reveal the emails starting flying asking what people thought of the new console design and specification. The almost universal answer was, 'I like the new controller, but the CPU looks a bit underpowered.'&quot; &lt;/p&gt; &lt;/div&gt;
  &lt;div class=&quot;image-gallery &quot;&gt;
    &lt;div class=&quot;illustration    &quot;&gt;
      &lt;img src=&quot;http://images.eurogamer.net/2013/articles//a/1/6/4/5/9/4/4/Nintendo_Wii_U_processor_heatspreader.jpg.jpg/EG11/resize/600x-1/quality/80/format/jpg&quot; width=&quot;600&quot; alt=&quot;processor&quot;&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
  &lt;p&gt;Almost immediately after the reveal the emails starting flying asking what people thought of the new console design and specification. The almost universal answer was, &quot;I like the new controller, but the CPU looks a bit underpowered&quot;.&lt;/p&gt;
  &lt;p&gt;Over the coming weeks people started doing other calculations trying to guess the performance of the machine - don't forget that this is a long time before development kits were available to do actual tests. Some people even built custom PC rigs with &lt;em&gt;under-clocked&lt;/em&gt; CPUs to try and gauge performance of their code on these machine. Again, the almost universal answer was that it wasn't going to be powerful enough to run next-gen engines and it might even struggle to do current-gen (PS3 and X360) titles. But in spite of these tests the management made the decision, for various business reasons, to release a game on the Wii U. So now we had to get stuck in and try to make a game.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;&quot;&gt;
  &lt;h2&gt;And so, to work&lt;/h2&gt;
  &lt;p&gt;Soon after the decision was made the development kits started arriving. As is usual for early hardware they were bigger than the final design with a mixture of connectors and ports used specifically for development. So we plugged them in and flashed them to the latest system code, then tried to get a simple &quot;hello world&quot; type game running, which proved harder than you might think.&lt;/p&gt;
  &lt;p&gt;Having worked on other hardware consoles, I suppose that we were rather spoilt by having mature toolchains that integrated nicely with our development environment. Wii U on the other hand seemed to be trying at every turn to make it difficult to compile and run any code. Nintendo had provided an integration of their development tools into Visual Studio - the de facto standard for development - but it didn't work, not even close. So time was spent trying to get this fixed up, while reporting the issue to the platform holder. Eventually we received a solution from Nintendo via another third-party company who had also been working on this issue for a while.&lt;/p&gt;
  &lt;p&gt;So now we could make the code visible in Visual Studio and get it compiling, which was good, but the compilation times were really slow, even for minor changes. Then it had to do the link step, at which point you could happily get up, make a cup of tea, have a chat and get back to your desk before the link was complete. Link times were measured in multiple (four or more) minutes on Wii U compared to around one minute on other platforms.&lt;/p&gt;
  &lt;p&gt;This doesn't sound bad, but when you are debugging and making lots of changes, these additional times add up. If you made 10 changes to a file in a morning, you could be spending over 50 minutes waiting for the linker to complete, which is a lot of wasted time. &lt;/p&gt;
  &lt;div class=&quot;pullquote  left &quot;&gt;&lt;p&gt;&quot;As a team, we lost days of time to the compile/link/debug overheads and this negatively impacted the amount of features that we could put into our game before the release date.&quot; &lt;/p&gt; &lt;/div&gt;
  &lt;div class=&quot;image-gallery &quot;&gt;
    &lt;div class=&quot;illustration    &quot;&gt;
      &lt;img src=&quot;http://images.eurogamer.net/2013/articles//a/1/6/4/5/9/4/4/wii_devkit.jpg.jpg/EG11/resize/600x-1/quality/80/format/jpg&quot; width=&quot;600&quot; height=&quot;224.922760041195&quot; alt=&quot;devkit&quot;&gt;
    &lt;/div&gt;
    
  &lt;/div&gt;
  &lt;p&gt;Finally, when you had the code, you would deploy it to the console and start up the debugger, which was part of the toolchain that Nintendo had licensed from Green Hills Software. As a seasoned developer I've used a lot of debuggers, but this one surprised even me. Its interface was clunky, it was very slow to use and if you made the mistake of actually clicking on any code, then it would pause and retrieve all of the values for the variables that you had clicked, which might take a minute or more to come back.&lt;/p&gt;
  &lt;p&gt;All of these things made the actual development of code harder than it should have been and ate into the development time of the game. As a team, we lost days of time to the compile/link/debug overheads and this negatively impacted the amount of features that we could put into our game before the release date.&lt;/p&gt;
  &lt;p&gt;Another curious thing to note at this point was that over the course of six months we received multiple different development kits in a variety of colours, none of which revealed why they were different from the previous one. We knew that there were some hardware bugs that were being fixed, but the release notes rarely stated what had changed - we just had to take the new ones and get them working with our code again, consuming valuable development time. There have been some interesting rumours circulating of PC-style development boxes, and even the Radeon HD 4850 (running underclocked) utilised as a proxy for the Wii U's GPU. We worked on Wii U from the early days and never saw equipment like this - our kits always took the form of custom hardware that I presume was based on near-to-final silicon.&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;&quot;&gt;&lt;h2&gt;Working with Wii U&lt;/h2&gt;&lt;p&gt;Now that the game was up and running on the console we could start developing features that would use the new controllers and make our game stand out on the platform. But soon after starting this we ran into some issues that the (minimal) documentation didn't cover, so we asked questions of our local Nintendo support team. They didn't know the answers so they said they would check with the developers in Japan and we waited for a reply. And we waited. And we waited.&lt;/p&gt;&lt;p&gt;After about a week of chasing we heard back from the support team that they had received an answer from Japan, which they emailed to us. The reply was in the form of a few sentences of very broken English that didn't really answer the question that we had asked in the first place. So we went back to them asking for clarification, which took another week or so to come back. After the second delay we asked why it was taking to long for replies to come back from Japan, were they very busy? The local support team said no, it's just that any questions had to be sent off for translation into Japanese, then sent to the developers, who replied and then the replies were translated back to English and sent back to us. With timezone differences and the delay in translating, this usually took a week !&lt;/p&gt;&lt;p&gt;Getting the game to run at its target frame-rate is a part of the development process that is less interesting in this context as it follows the standard pattern. Get the game running, optimise the code (CPU and GPU) and if it still won't perform, cut back on features until it does fit.&lt;/p&gt;&lt;div class=&quot;pullquote  left &quot;&gt;&lt;p&gt;&quot;Getting the game to run at its target frame-rates... follows the standard pattern. Get the game running, optimise the code and if it still won't perform, cut back on features until it does fit.&quot; &lt;/p&gt; &lt;/div&gt;				&lt;p class=&quot;video&quot;&gt;
					&lt;iframe width=&quot;600&quot; height=&quot;338&quot; src=&quot;http://www.youtube.com/embed/ABBp3s0ilmY&quot;&gt;&lt;/iframe&gt;
					
				&lt;/p&gt;
&lt;p&gt;As far as the CPU optimisations went, yes we did have to cut back on some features due to the CPU not being powerful enough. As we originally feared, trying to support a detailed game running in HD put a lot of strain on the CPUs and we couldn't do as much as we would have liked. Cutting back on some of the features was an easy thing to do, but impacted the game as a whole. Code optimised for the PowerPC processors found in the Xbox 360 and PlayStation 3 wasn't always a good fit for the Wii U CPU, so while the chip has some interesting features that let the CPU punch above its weight, we couldn't fully take advantage of them. However, some code could see substantial improvements that did mitigate the lower clocks - anything up to a 4x boost owing to the removal of Load-Hit-Stores, and higher IPC (instructions per cycle) via the inclusion of out-of-order execution. &lt;/p&gt;&lt;p&gt;On the GPU side, the story was reversed. The GPU proved very capable and we ended up adding additional &quot;polish&quot; features as the GPU had capacity to do it. There was even some discussion on trying to utilise the GPU via compute shaders (GPGPU) to offload work from the CPU - exactly the approach I expect to see gain traction on the next-gen consoles - but with very limited development time and no examples or guidance from Nintendo, we didn't feel that we could risk attempting this work. If we had a larger development team or a longer timeframe, maybe we would have attempted it, but in hindsight we would have been limited as to what we could have done before we maxed out the GPU again. The GPU is better than on PS3 or Xbox 360, but leagues away from the graphics hardware in the PS4 or Xbox One.&lt;/p&gt;&lt;p&gt;I've also seen some concerns about the utilisation of DDR3 RAM on Wii U, and a bandwidth deficit compared to the PS3 and Xbox 360. This wasn't really a problem for us. The GPU could fetch data rapidly with minimal stalls (via the EDRAM) and we could efficiently pre-fetch, allowing the GPU to run at top speed.&lt;/p&gt;&lt;/section&gt;&lt;section class=&quot;&quot;&gt;&lt;h2&gt;Nintendo vs. online gaming&lt;/h2&gt;&lt;p&gt;Now that the game was coming together and the hardware issues were being resolved our attention turned to the networking side of our game and its interface to the newly announced Nintendo Network. We spotted early on that there seemed to be gaps in the documentation, and the code, around the networking area, so we asked for clarification. After the usual translation delay we received word that they were still working on the code, but don't worry it would be arriving soon.&lt;/p&gt;&lt;p&gt;Alarm bells started ringing quietly in my head again, but I put them to one side for the time being. This is Nintendo's new network infrastructure that they are basing their console around, they should make sure that it is complete and fully tested before sharing it, so I could forgive them some delay. We had the basics so we could at least do some testing and connect multiple kits together, but a lot of the Mii and friends content was missing and there was no way to test how the existing code would behave in a &quot;retail environment&quot; as there was no retail &quot;flash&quot; for the development kits. We had to code it all in the dark and just hope that it worked.&lt;/p&gt;&lt;p&gt;Around this time we got the chance to talk to some more senior people in Nintendo, via a phone conference, as they were gathering feedback on our development experiences and their toolchain. This phone conference gave an interesting insight into Nintendo and how it appears to operate.&lt;/p&gt;&lt;div class=&quot;pullquote  left &quot;&gt;&lt;p&gt;&quot;At some point in this conversation we were informed that it was no good referencing Live and PSN as nobody in [Nintendo's] development teams used those systems (!) so could we provide more detailed explanations for them?&quot; &lt;/p&gt; &lt;/div&gt;				&lt;p class=&quot;video&quot;&gt;
					&lt;iframe width=&quot;600&quot; height=&quot;338&quot; src=&quot;http://www.youtube.com/embed/doMx1UEKEzU&quot;&gt;&lt;/iframe&gt;
					
				&lt;/p&gt;
&lt;p&gt;The discussion started off well enough and covered off our experiences with the hardware and (slow) toolchain and then we steered them towards discussing when the online features might be available. We were told that the features, and the OS updates to support them, would be available before the hardware launch, but only just. There were apparently issues with setting up a large networking infrastructure to rival Sony and Microsoft that they hadn't envisaged.&lt;/p&gt;&lt;p&gt;This was surprising to hear, as we would have thought that they had plenty of time to work on these features as it had been announced months before, so we probed a little deeper and asked how certain scenarios might work with the Mii friends and networking, all the time referencing how Xbox Live and PSN achieve the same thing. At some point in this conversation we were informed that it was no good referencing Live and PSN as nobody in their development teams used those systems (!) so could we provide more detailed explanations for them? My only thought after this call was that they were struggling - badly - with the networking side as it was far more complicated than they anticipated. They were trying to play catch-up with the rival systems, but without the years of experience to back it up.&lt;/p&gt;&lt;p&gt;As promised, (just) before the worldwide launch we received the final networking features that we required for our game along with an OS update for the development kits that would allow us to test. So we patched up our code and tried to start testing our game.&lt;/p&gt;&lt;p&gt;First up we had to flash the kits to the retail mode that had the Mii and network features. This was a very complicated manual process that left the consoles in a halfway state. In the retail mode we could test our features and ensure that they worked as expected, which would be a requirement for getting through Nintendo certification, but in this mode the debugging capabilities were limited. So we could see when things went wrong, but we couldn't fully debug to find out why. As developers, we had to make a choice and hope that any issues that you found were due to the (untested) OS code and wouldn't happen in the final retail environment. What should have been simple tasks were long-winded and error prone. Simple things like sending a friends request to another user were not supported in the OS, so you had to boot a separate program on the console manually, via a debug menu, so that you could send one. But if any error occurred there was no way to debug why it had failed, it just failed.&lt;/p&gt;&lt;p&gt;We started to ask questions about how they could possibly launch the console, which was a matter of weeks away, with a partially developed OS. How were they going to get the OS onto all of the consoles that had been manufactured up to that point? Was it just that we got it late, but they had pushed it into the production line earlier?&lt;/p&gt;&lt;p&gt;Launch day came around and the answer became clear: Nintendo was late - very late - with its network systems. In fact, the only way to access their systems fully was to download a &lt;em&gt;big&lt;/em&gt; patch on day one that added all these missing components. Without that patch a lot of the release titles would have been only semi-functional.&lt;/p&gt;&lt;div class=&quot;pullquote  left &quot;&gt;&lt;p&gt;&quot;We started to ask questions about how they could possibly launch the console, which was a matter of weeks away, with a partially developed OS... Launch day came around and the answer became clear: Nintendo was late - very late - with its network systems.&quot; &lt;/p&gt; &lt;/div&gt;				&lt;p class=&quot;video&quot;&gt;
					&lt;iframe width=&quot;600&quot; height=&quot;338&quot; src=&quot;http://www.youtube.com/embed/1rUR9A0Dl00&quot;&gt;&lt;/iframe&gt;
					
				&lt;/p&gt;
&lt;/section&gt;&lt;section class=&quot;&quot;&gt;&lt;h2&gt;What happened next?&lt;/h2&gt;&lt;p&gt;Well, we eventually released our game and it was generally well-received, so the management sat back to see what kind of sales figures we would get for all our efforts. Without going into detail it would be fair to say that the numbers we were seeing were less than impressive. In fact we would be lucky to make back all the money that we had invested in making the game in the first place, and although the management publicly supported the Wii U platform, it is unlikely that we would ever release another Wii U title.&lt;/p&gt;&lt;p&gt;But what about the rest of the world? How had other development studios faired? The story of what happened next is pretty well documented in the gaming press, but I'd like to highlight some interesting points that have been on my mind recently. Firstly, third-party support. Do you remember all the hype surrounding the Wii U launch? All those third parties showing videos of existing games that they were going to bring to the Wii U? Whatever happened to a lot of those games?&lt;/p&gt;&lt;p&gt;After the initial flurry of game titles a lot of the studios quietly backed away from their initial statements and announced, with minimal press, that they were in fact not going to make a Wii U version. The reasons behind a particular title not appearing on the Wii U are all pure speculation, but I personally think that a combination of:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Previous development experience using the toolchain and hardware put off development teams from making another title on Wii U.&lt;/li&gt;&lt;li&gt;The technical and feature support from Nintendo were lacking for third-party studios. There was a feeling internally that if you weren't a first-party development studio, you were largely ignored by Nintendo, as we were superficial to their profits. Internally developed titles would save Nintendo and we were just there to add depth to the games catalogue.&lt;/li&gt;&lt;li&gt;The sales figures for the Wii U console were not looking that good soon after launch. There was a lot of confusion in the general population around the launch as most people thought that the Wii U was some kind of add-on to the Wii, they didn't know that it was a new console. This lack of awareness probably contributed to the console not getting off to the start that Nintendo would have hoped and put off studio from developing on the hardware.&lt;/li&gt;&lt;li&gt;Nintendo also fell victim to bad timing. A few months after the console launched the next-gen hype train stepped up a gear as Sony announced the PlayStation 4, with Microsoft joining the fray a few months later. Don't forget that many of the larger studios would have known about the hardware months before it was announced, well before the Wii U hardware actually launched.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;So, these larger studios had a choice. Would they develop a port of an existing game to a console with limited capabilities and limited market penetration? Or put their teams to work on developing new features and concepts for the &quot;real&quot; next-gen consoles that were going to be launched that year? When you look at it this way, the choice isn't that hard.&lt;/p&gt;&lt;div class=&quot;pullquote  left &quot;&gt;&lt;p&gt;&quot;Larger studios had a choice. Would they port of an existing game to a console with limited capabilities and limited market penetration? Or put their teams to work on developing new features and concepts for the 'real' next-gen consoles?&quot; &lt;/p&gt; &lt;/div&gt;				&lt;p class=&quot;video&quot;&gt;
					&lt;iframe width=&quot;600&quot; height=&quot;338&quot; src=&quot;http://www.youtube.com/embed/yvg9mwBrZy4&quot;&gt;&lt;/iframe&gt;
					
				&lt;/p&gt;
&lt;p&gt;From a first-party perspective, it seems that Nintendo itself hasn't had the easiest time. Now this is pure speculation, but from interactions with some of the development teams it seems as though Nintendo's own teams were having real troubles adapting to the new console - the main reason being the move to HD and the ability of the hardware to support it. Don't forget that until the Wii U came out, none of the first-party titles were in HD and the move from SD to HD is not as easy as you would expect. PS3 and Xbox 360 developers went through this pain early in the previous console cycle and it cost them a lot of time and money trying to adapt, with some studios failing in a big way.&lt;/p&gt;&lt;p&gt;Nintendo's internal teams were now facing this challenge on a new console with limited development time and a lot of pressure to deliver compelling titles. With these pressures upon them it was inevitable that some of the higher-profile titles would slip, but it's surprising how sparse the first-party line-up has been over the last year.&lt;/p&gt;&lt;/section&gt;&lt;section class=&quot;&quot;&gt;
  &lt;h2&gt;The future for Wii U&lt;/h2&gt;
  &lt;p&gt;Can the Wii U compete in this brave new world of next-gen (current-gen?) consoles? In terms of raw performance it sits uncomfortably between the previous generation and the current one. Parts of the hardware run better than the previous generation, but other parts drag it down. If you tried to compare the Wii U against the PS4/XO, it comes off very badly indeed - it just cannot compete with the new consoles.&lt;/p&gt;
  &lt;p&gt;At a very basic level, look at the power draw taken by the next-gen consoles compared to the Wii U. The PlayStation 4 draws over 100W more from the mains than Nintendo's console, and it does so using the latest, most power-efficient x86 cores from AMD in concert with a much larger GPU that's a generation ahead and runs on a much smaller fabrication process - 28nm vs. what I'm reliably informed is the 55nm process from Japanese company Renasas. &lt;/p&gt;
  &lt;p&gt;There are some fleeting parallels between Wii U and the next-gen consoles - the combination of a low-power CPU with a much more powerful graphics chip - but the notion of next-gen titles being easily portable to the Wii U just doesn't work. The gulf in power is just too high, while the GPGPU that we'll see on Xbox One and PlayStation 4 isn't compatible with the older shader model four hardware found in the Wii U.&lt;/p&gt;
  &lt;p&gt;Doubtless, the first-party developers at Nintendo will make the hardware sing - they always do - but the situation looks grim for those of us in third-party development, with the opportunity to progress on the hardware held back by both the quality of the tools and the lack of financial reward for tailoring our code to the strengths of the hardware. So where does that leave the Wii U?&lt;/p&gt;
  &lt;p&gt;Personally I'm not sure on what will happen, but if the current trends continue, the Wii U will probably continue to sell in small quantities until a &quot;must have&quot; title is released, probably from a first-party studio, at which point the sales will sky rocket for a while - but even so, matching the momentum of PlayStation 4 and Xbox One seems highly unlikely. Other variables such as the recent news regarding China lifting the ban on games consoles may influence Nintendo's future direction. This huge untapped market may provide a lifeline in terms of sales, but with the low wages of the general population these sales might well come from the original Wii, rather than the more expensive Wii U.&lt;/p&gt;
  &lt;p&gt;You can never discount Nintendo, but based on my experience - and the sales of the platform - the company's facing its most testing challenge in modern times.&lt;/p&gt;
&lt;/section&gt;
		

	
																																												
	
						
						
	
					&lt;/article&gt;
	
											
				
									&lt;/div&gt;
</description>
<title>
The Secret Developers: Wii U - the inside story
</title>
</item>
</channel>
</rss>
